{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\E'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\E'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_3956\\348781867.py:5: SyntaxWarning: invalid escape sequence '\\E'\n",
      "  sys.path.append(os.path.abspath(\"E:\\EmailTool-V2\\Barzaan\\email_creator_app\"))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of the module to sys.path\n",
    "sys.path.append(os.path.abspath(\"E:\\EmailTool-V2\\Barzaan\\email_creator_app\"))\n",
    "from database_training.database_process_mutifile import process_files_and_save_output\n",
    "from database_training.merger_for_pckl import update_pkl_with_csv\n",
    "import pandas as pd\n",
    "from database_training.update_db_manually import update_pickle\n",
    "from email_creation.create_emails import email_creator_app\n",
    "from database_training.database_process_single_file import process_single_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: e:\\EmailTool-V2\\Barzaan\\email_creator_app\n",
      "Base directory: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/\n",
      "old_db_pickle_file_path directory: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/main_database//old_db.pkl\n",
      "New 48734\n",
      "Old 1045691\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "# Get the full path of the script\n",
    "script_path = os.path.abspath('test-notebook.ipynb')\n",
    "# Get the directory containing the script\n",
    "user_path = os.path.dirname(script_path)\n",
    "print(\"Base directory:\", user_path)\n",
    "base_path = user_path+\"/files/\" \n",
    "print(\"Base directory:\", base_path)\n",
    "Source_file_path = base_path+'database_source_files'\n",
    "Output_files_path = base_path+'database_output_files'\n",
    "new_db_pickle_file_path = base_path+'main_database//'+'new_db.pkl'\n",
    "old_db_pickle_file_path = base_path+'main_database//'+'old_db.pkl'\n",
    "print(\"old_db_pickle_file_path directory:\", old_db_pickle_file_path)\n",
    "# Read the pickle file\n",
    "with open(new_db_pickle_file_path, \"rb\") as file:\n",
    "    new_data_dict = pickle.load(file)  # Load as a dictionary\n",
    "\n",
    "# Print or use the dictionary\n",
    "print(\"New\", len(new_data_dict))\n",
    "# Read the pickle file\n",
    "with open(old_db_pickle_file_path, \"rb\") as file:\n",
    "    data_dict = pickle.load(file)  # Load as a dictionary\n",
    "\n",
    "# Print or use the dictionary\n",
    "print(\"Old\", len(data_dict))\n",
    "#1043984\n",
    "# New 47021\n",
    "# Old 1043989\n",
    "# New 47021\n",
    "# Old 1043989\n",
    "# New 48176\n",
    "# Old 1045119\n",
    "# New 48356\n",
    "# Old 1045298 \n",
    "\n",
    "\n",
    "# better youth\n",
    "\n",
    "\n",
    "# New 48356 2apriil 11.30pm\n",
    "# Old 1045298\n",
    "\n",
    "\n",
    "# New 48734\n",
    "# Old 1045690\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th sfg : LastName@soc.mil\n",
      "magellan of nevada : FirstInitialLastName@magellanhealth.com\n",
      "magellan pipe line : FirstName.LastName@7n.com\n",
      "maggie shops for you : FirstInitialLastName@hu-friedy.com\n",
      "maggie speaks : FirstName.LastName@7n.com\n",
      "10th sfg : LastName@soc.mil\n",
      "magellan of nevada : FirstInitialLastName@magellanhealth.com\n",
      "magellan pipe line : FirstName.LastName@7n.com\n",
      "maggie shops for you : FirstInitialLastName@hu-friedy.com\n",
      "maggie speaks : FirstName.LastName@7n.com\n",
      "48734\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 key-value pairs from the dictionary\n",
    "for i, (key, value) in enumerate(new_data_dict.items()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(key, \":\", value)\n",
    "\n",
    "\n",
    "\n",
    "# Convert list values to normal strings\n",
    "normalized_dict = {key: value[0] if isinstance(value, list) and len(value) == 1 else value for key, value in new_data_dict.items()}\n",
    "\n",
    "# Print normalized dictionary\n",
    "# Print the first 5 key-value pairs from the dictionary\n",
    "for i, (key, value) in enumerate(normalized_dict.items()):\n",
    "    if i == 5:\n",
    "        break\n",
    "    print(key, \":\", value)\n",
    "\n",
    "\n",
    "# Save the modified data back to a pickle file (optional)\n",
    "with open(new_db_pickle_file_path, \"wb\") as f:\n",
    "    pickle.dump(normalized_dict, f)\n",
    "print (len(normalized_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the pickle file\n",
    "with open(new_db_pickle_file_path, \"rb\") as f:\n",
    "    data_dict = pickle.load(f)\n",
    "\n",
    "# Extract unique company names (dictionary keys)\n",
    "unique_companies = list(data_dict.keys())\n",
    "\n",
    "# Save to a text file\n",
    "with open(\"new_db_pickle.txt\", \"w\") as f:\n",
    "    for i, company in enumerate(unique_companies, start=1):\n",
    "        f.write(f\"{i} {company}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FirstName@betteryou.uk.com FirstName@betteryou.uk.com\n"
     ]
    }
   ],
   "source": [
    "k ='10th SFG'\n",
    "k ='better youth'\n",
    "print (new_data_dict[str.lower(k)], data_dict[str.lower(k)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'output.csv' has been created successfully with UTF-8 encoding.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# import csv\n",
    "\n",
    "# # Load the pickle file\n",
    "# with open(new_db_pickle_file_path, \"rb\") as pkl_file:\n",
    "#     data = pickle.load(pkl_file)\n",
    "\n",
    "# # Ensure data is a dictionary\n",
    "# if isinstance(data, dict):\n",
    "#     # Write to CSV with UTF-8 encoding\n",
    "#     with open(\"newdb_output.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
    "#         writer = csv.writer(csv_file)\n",
    "#         writer.writerow([\"Key\", \"Value\"])  # Header\n",
    "#         for key, value in data.items():\n",
    "#             writer.writerow([key, value])\n",
    "\n",
    "#     print(\"CSV file 'output.csv' has been created successfully with UTF-8 encoding.\")\n",
    "# else:\n",
    "#     print(\"The loaded pickle file does not contain a dictionary.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted key: White Oak Ventures\n",
      "Updated pickle file saved.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Load the pickle file\n",
    "# with open(old_db_pickle_file_path, \"rb\") as pkl_file:\n",
    "#     data = pickle.load(pkl_file)\n",
    "\n",
    "# # Ensure data is a dictionary\n",
    "# if isinstance(data, dict):\n",
    "#     key_to_delete = 'White Oak Ventures'  # Replace with the actual key you want to remove\n",
    "    \n",
    "#     # Remove the key if it exists\n",
    "#     if key_to_delete in data:\n",
    "#         del data[key_to_delete]\n",
    "#         print(f\"Deleted key: {key_to_delete}\")\n",
    "#     else:\n",
    "#         print(f\"Key '{key_to_delete}' not found in the dictionary.\")\n",
    "\n",
    "#     # Save the updated dictionary back to the pickle file\n",
    "#     with open(old_db_pickle_file_path, \"wb\") as pkl_file:\n",
    "#         pickle.dump(data, pkl_file)\n",
    "\n",
    "#     print(\"Updated pickle file saved.\")\n",
    "# else:\n",
    "#     print(\"The pickle file does not contain a dictionary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old 1043985\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FirstName.LastName@sexyhair.com'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory: e:\\EmailTool-V2\\Barzaan\\email_creator_app\n",
      "Base directory: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/\n",
      "AustinTxseggrigation_1_missingcheckpoint.txt\n",
      "last_index: 0\n",
      "Batch:: 0_9\n",
      "printing lenghth 9\n",
      "Number of batch: 2  and each batch have  5  many rows \n",
      "2025-03-30\n",
      "Batch-------------------------------------- 0\n",
      "sleeping after batch\n",
      "Index(['company', 'email pattern'], dtype='object')\n",
      "process csv\n",
      "file process over\n",
      "****************Pickle file updated: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/main_database//old_db.pkl\n",
      "process csv\n",
      "file process over\n",
      "****************Pickle file updated: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/main_database//new_db.pkl\n",
      "e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/automatic_emails_format_created/Batch0_9__SCRAPPER__0_2025-03-30.csv\n",
      "Batch-------------------------------------- 1\n",
      "sleeping after batch\n",
      "Index(['company', 'email pattern'], dtype='object')\n",
      "process csv\n",
      "file process over\n",
      "****************Pickle file updated: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/main_database//old_db.pkl\n",
      "process csv\n",
      "file process over\n",
      "****************Pickle file updated: e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/main_database//new_db.pkl\n",
      "e:\\EmailTool-V2\\Barzaan\\email_creator_app/files/automatic_emails_format_created/Batch0_9__SCRAPPER__1_2025-03-30.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Get the full path of the script\n",
    "script_path = os.path.abspath('appy.py')\n",
    "user_path = os.path.dirname(script_path)\n",
    "print(\"Base directory:\", user_path)\n",
    "base_path = user_path+\"/files/\"\n",
    "print(\"Base directory:\", base_path)\n",
    "Source_file_path = base_path+'database_source_files'\n",
    "Output_files_path = base_path+'database_output_files'\n",
    "new_db_pickle_file_path = base_path+'main_database//'+'new_db.pkl'\n",
    "old_db_pickle_file_path = base_path+'main_database//'+'old_db.pkl'\n",
    "email_created_path = base_path+'created_emails'\n",
    "\n",
    "scrapper_output = base_path + 'automatic_emails_format_created'\n",
    "\n",
    "missing_data = base_path+'missing_emails'\n",
    "verified_path = base_path+'verified_emails'\n",
    "from automatic_email_format import scrapper_run\n",
    "scrapper_run.app_run(r'E:\\EmailTool-V2\\Barzaan\\email_creator_app\\files\\missing_emails\\AustinTxseggrigation_1_missing.csv',old_db_pickle_file_path,new_db_pickle_file_path,scrapper_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 3\n"
     ]
    }
   ],
   "source": [
    "dfss = pd.read_csv(r'E:\\EmailTool-V2\\Barzaan\\email_creator_app/files/automatic_emails_format_created/Batch0_6500__3AprSCRAPPER__0_2025-04-03.csv')\n",
    "unique_companies = dfss[\"company\"].unique()  # Extract unique company names\n",
    "total_unique = unique_companies.shape[0] \n",
    "print(\"Total results:\", total_unique)  # Print the shape of the unique companies array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged pickle file saved as: E:\\EmailTool-V2\\Barzaan\\email_creator_app\\files\\merged_data.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "# Folder containing the pickle files\n",
    "folder_path = r\"E:\\EmailTool-V2\\Barzaan\\email_creator_app\\files\"  # Change this to your actual folder path\n",
    "\n",
    "# Initialize an empty dictionary to store merged data\n",
    "merged_data = {}\n",
    "\n",
    "# Loop through all .pkl files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pkl\"):  # Process only .pkl files\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        # Load the pickle file\n",
    "        with open(file_path, \"rb\") as pkl_file:\n",
    "            data = pickle.load(pkl_file)\n",
    "            \n",
    "            # Ensure the file contains a dictionary\n",
    "            if isinstance(data, dict):\n",
    "                for key, value in data.items():\n",
    "                    if key not in merged_data:  # Add only if the key is not already present\n",
    "                        merged_data[key] = value\n",
    "            else:\n",
    "                print(f\"Skipping {file_name}: Not a dictionary.\")\n",
    "\n",
    "# Save the merged data into a new pickle file\n",
    "output_file = os.path.join(folder_path, \"merged_data.pkl\")\n",
    "with open(output_file, \"wb\") as pkl_file:\n",
    "    pickle.dump(merged_data, pkl_file)\n",
    "\n",
    "print(f\"Merged pickle file saved as: {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
